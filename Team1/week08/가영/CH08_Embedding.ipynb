{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9M7A-iKlahE"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 코랩 비밀번호 설정에 저장된 키를 가져옵니다.\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain_openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yrMWlq-g-Bh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAIEmbeddings\n",
        "대표적인 사전 학습된 언어 모델: BERT와 GPT"
      ],
      "metadata": {
        "id": "TYuV_OHp0gl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# OpenAI의 \"text-embedding-3-large??\" 모델을 사용하여 임베딩을 생성합니다.\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "text = \"임베딩 테스트를 하기 위한 샘플 문장입니다.\""
      ],
      "metadata": {
        "id": "YWIrIEHc6MBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트를 임베딩하여 쿼리 결과를 생성합니다. -> 리스트\n",
        "query_result = embeddings.embed_query(text)"
      ],
      "metadata": {
        "id": "sLF6bcc-9TEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 쿼리 결과의 처음 5개 항목을 선택합니다.\n",
        "query_result[:5]"
      ],
      "metadata": {
        "id": "leiGNRb990q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_result = embeddings.embed_documents(\n",
        "    [text]\n",
        ")  # 텍스트를 임베딩하여 문서 벡터를 생성합니다.\n"
      ],
      "metadata": {
        "id": "k3VBsW50_YVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 결과의 첫 번째 요소에서 처음 5개 항목을 선택합니다.\n",
        "doc_result[0][:5]\n"
      ],
      "metadata": {
        "id": "vxi4Y_lEBz9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "차원(dimensions) 조정"
      ],
      "metadata": {
        "id": "uBu9cBkCFnms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 결과의 첫 번째 요소의 길이를 반환합니다.\n",
        "len(doc_result[0])"
      ],
      "metadata": {
        "id": "UxEZ8lH_EY2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1536 차원-> 1024 차원으로 축소\n",
        "\n",
        "# OpenAI의 \"text-embedding-3-small\" 모델을 사용하여 1024차원의 임베딩을 생성하는 객체를 초기화합니다.\n",
        "embeddings_1024 = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)\n",
        "# 주어진 텍스트를 임베딩하고 첫 번째 임베딩 벡터의 길이를 반환합니다.\n",
        "len(embeddings_1024.embed_documents([text])[0])"
      ],
      "metadata": {
        "id": "ucp61wnlFi3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "유사도 계산"
      ],
      "metadata": {
        "id": "Ijr7VnULGDcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
        "sentence2 = \"안녕하세요? 반갑습니다!\"\n",
        "sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
        "sentence4 = \"Hi, nice to meet you.\"\n",
        "sentence5 = \"I like to eat apples.\"\n",
        "\n",
        "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n",
        "embedded_sentences = embeddings_1024.embed_documents(sentences)"
      ],
      "metadata": {
        "id": "-VHgugw4GJL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    return cosine_similarity([a], [b])[0][0]\n",
        "\n",
        "for i, sentence in enumerate(embedded_sentences):\n",
        "    for j, other_sentence in enumerate(embedded_sentences):\n",
        "        if i < j:\n",
        "            print(\n",
        "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
        "            )"
      ],
      "metadata": {
        "id": "955Onp5RGQJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 캐시 임베딩"
      ],
      "metadata": {
        "id": "1lNIcq4fG9tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r https://raw.githubusercontent.com/teddylee777/langchain-kr/main/requirements-mini.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WT_l2gmkUqrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.storage import LocalFileStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "# OpenAI 임베딩을 사용하여 기본 임베딩 설정\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# 로컬 파일 저장소 설정\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "# 캐시를 지원하는 임베딩 생성\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    underlying_embeddings=embedding,\n",
        "    document_embedding_cache=store,\n",
        "    namespace=embedding.model  # 기본 임베딩과 저장소를 사용하여 캐시 지원 임베딩을 생성\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e8oRsGyJLW3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store에서 키들을 순차적으로 가져옵니다.\n",
        "list(store.yield_keys())"
      ],
      "metadata": {
        "id": "G43etHKpHBZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# 문서 로드\n",
        "raw_documents = TextLoader(\"./data/appendix-keywords.txt\").load()\n",
        "# 문자 단위로 텍스트 분할 설정\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "# 문서 분할\n",
        "documents = text_splitter.split_documents(raw_documents)"
      ],
      "metadata": {
        "id": "wHABygUbTPOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 실행 시간을 측정합니다.\n",
        "%time db = FAISS.from_documents(documents, cached_embedder)  # 문서로부터 FAISS 데이터베이스 생성"
      ],
      "metadata": {
        "id": "aPbjPbJXTQw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 허깅페이스 임베딩"
      ],
      "metadata": {
        "id": "Mn_mWQWhapdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "5ITtfKD3as94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "texts = [\n",
        "    \"안녕, 만나서 반가워.\",\n",
        "    \"LangChain simplifies the process of building applications with large language models\",\n",
        "    \"랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \",\n",
        "    \"LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\",\n",
        "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "PnxG57e6a59P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Endpoint Embedding"
      ],
      "metadata": {
        "id": "iJ8hC9Fsw7_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-community \\\n",
        "    huggingface_hub"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4_NOAqwa0uMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82314d01"
      },
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 코랩 비밀번호 설정에 저장된 Hugging Face API 키를 가져옵니다.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=model_name,\n",
        "    task=\"feature-extraction\",\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        ")"
      ],
      "metadata": {
        "id": "NUV10nfIw7zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Document Embedding 수행\n",
        "embedded_documents = hf_embeddings.embed_documents(texts)"
      ],
      "metadata": {
        "id": "TvBkXcAUii5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Embedding 수행\n",
        "embedded_query = hf_embeddings.embed_query(\"LangChain 에 대해서 알려주세요.\")"
      ],
      "metadata": {
        "id": "4-KvsfVO5KXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[HuggingFace Endpoint Embedding]\")\n",
        "print(f\"Model: \\t\\t{model_name}\")\n",
        "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
      ],
      "metadata": {
        "id": "735EujOO5DwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### query 와 embedding_document 간의 유사도 계산\n",
        "이 코드는 **\"벡터 검색 엔진이 내부적으로 어떤 수학적 원리로 작동하는가\"**를 보여주기 위한 '원리 증명용' 코드입니다.\n",
        "\n",
        "실제로 서비스를 개발할 때는 Chroma같은 '벡터 저장소(Vector Store)' 라이브러리를 사용"
      ],
      "metadata": {
        "id": "kVTb9wRT7B35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
        "np.array(embedded_query) @ np.array(embedded_documents).T\n",
        "#@ 연산 (행렬 곱)"
      ],
      "metadata": {
        "id": "Q6ZXo5rn7CpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]\n",
        "sorted_idx"
      ],
      "metadata": {
        "id": "v9i1PBgI8Uhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
        "for i, idx in enumerate(sorted_idx):\n",
        "    print(f\"[{i}] {texts[idx]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "8vDOqVLn8YC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Embeddings"
      ],
      "metadata": {
        "id": "EMxt4Kfb8bJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \\\n",
        "    transformers \\\n",
        "    peft \\\n",
        "    sentence-transformers"
      ],
      "metadata": {
        "id": "QTGeodf8hlIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
        "# model_name = \"intfloat/multilingual-e5-large\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\": \"cuda\"},  # cuda, cpu\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "%time\n",
        "# Document\n",
        "embedded_documents1 = hf_embeddings.embed_documents(texts)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XiSOiUjO8vJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BGE-M3 임베딩"
      ],
      "metadata": {
        "id": "leSjHmZFiPh-"
      }
    }
  ]
}